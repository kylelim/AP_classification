{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_openai\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "import requests\n",
    "\n",
    "def getMlopKey():\n",
    "    url = os.getenv(\"KEYCLOAK_TOKEN_URL\")\n",
    "    client_id = os.getenv(\"KEYCLOAK_CLIENT_ID\")\n",
    "    client_secret = os.getenv(\"KEYCLOAK_CLIENT_SECRET\")\n",
    "    header = {\"Request-client\": \"IAM_PORTAL\", \"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "    data = {\"grant_type\": \"client_credentials\", \"client_id\": client_id, \"client_secret\": client_secret}\n",
    "    response = requests.post(url, headers=header, data=data)\n",
    "    responseJson = response.json()\n",
    "    access_token = responseJson['access_token']\n",
    "    os.environ['MLOP_ACCESS_TOKEN'] = access_token\n",
    "\n",
    "def set_env_var(): #to be filled\n",
    "        \n",
    "        os.environ[\"KEYCLOAK_BASE_URL\"] = \"\" \n",
    "        os.environ[\"KEYCLOAK_CLIENT_ID\"] = \"\"\n",
    "        os.environ[\"KEYCLOAK_CLIENT_SECRET\"] = \"\"\n",
    "        os.environ[\"KEYCLOAK_REALM\"] = \"\"\n",
    "        os.environ[\"KEYCLOAK_TOKEN_URL\"] = \"\"\n",
    "        os.environ[\"ENV\"] = \"local\"\n",
    "        os.environ[\"MLOP_HOST\"] = \"\"\n",
    "\n",
    "\n",
    "def get_llm_client():\n",
    "    set_env_var()\n",
    "    getMlopKey()\n",
    "    llmClient = AzureChatOpenAI(\n",
    "                deployment_name=\"\",\n",
    "                azure_endpoint=os.getenv(\"MLOP_HOST\"),\n",
    "                openai_api_version=\"2024-02-01\",\n",
    "                temperature=0.8,\n",
    "                openai_api_key=os.getenv(\"MLOP_ACCESS_TOKEN\"),\n",
    "    )\n",
    "    \n",
    "    return llmClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<background>\n",
      "In the vessel shipping industry, precise communication and accurate information sharing are paramount. The task at hand involves evaluating the relevance of a user input question and an email reply generated by an intern, based on five key dimensions: Semantic Relatedness, Contextual Alignment, Specificity and Detail, Information Sought, and Temporal Aspects. This evaluation is crucial for ensuring that inquiries and responses are effectively matched, providing valuable insights into the efficiency and reliability of internal communication processes within the shipping sector.\n",
      "\n",
      "</background>\n",
      "<Task Instruction>\n",
      "1. **Understand the Dimensions**: Begin by familiarizing yourself with the five dimensions of evaluation: Semantic Relatedness, Contextual Alignment, Specificity and Detail, Information Sought, and Temporal Aspects.\n",
      "2. **Analyze the User Input and Email Reply**: Examine the given user input question and the intern's email reply. Understand their content, intent, and the information they convey.\n",
      "3. **Evaluate Each Dimension Individually**:\n",
      "    - For **Semantic Relatedness**, assess how the meanings of the question and reply relate to each other, regardless of the vocabulary used.\n",
      "    - In **Contextual Alignment**, determine if both the question and reply pertain to the same domain or context, like legal procedures in shipping.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_enquiry = \"\"\"\n",
    "\"\"\"\n",
    " \n",
    "system_prompt = \"\"\"      \n",
    "    <Background>\n",
    "    - You are a specialized prompt engineer at a company focusing on vessel shipping services.\n",
    "    - Your core responsibility is to translate customer enquiries into a format that a large language model can understand and address with high accuracy and efficiency.\n",
    "    </Background>\n",
    "    <Task Instruction>\n",
    "    To utilize the assistant effectively, adhere to these steps:\n",
    "    1. Fully Grasp the Enquiry: Thoroughly understand the <user_enquiry> in its entirety.\n",
    "    2. Fill Out the Template: Carefully complete the 'Background', 'Task Instruction', 'Output Guideline', and 'Example' sections in the template, based on the <user_enquiry>.\n",
    "    - Background: Include comprehensive context and clearly define the assistant's role.\n",
    "    - Task Instruction: Provide detailed, step-by-step instructions tailored to resolving the enquiry.\n",
    "    - Output Guideline: Specify explicit output guidelines, including the required format, tone, and any other relevant details.\n",
    "    - Example: Provide specific, relevant examples from the vessel shipping industry to aid understanding.\n",
    "    3. Ensure Large Language Model Compatibility: Confirm that the 'prompt' translates the business enquiry into a clear, actionable format that a large language model can easily comprehend and address effectively.\n",
    "    4. Ensure the prompt uses precise and unambiguous language\n",
    "    </Task Instruction>\n",
    "    <Output Guideline>\n",
    "    - Only output the content within the <template>.\n",
    "    - Ensure the 'prompt' facilitates the translation of business enquiries into a format easily understood by the large language model.\n",
    "    <user_enquiry>\n",
    "    {user_enquiry}\n",
    "    </user_enquiry>\n",
    "    <Template>\n",
    "        <background>\n",
    " \n",
    "        </background>\n",
    "        <Task Instruction>\n",
    " \n",
    "        </Task Instruction>\n",
    "        <Output Guideline>\n",
    " \n",
    "        </Output Guideline>\n",
    "        <Example>\n",
    " \n",
    "        </Example>\n",
    "    </Template>\n",
    "\"\"\"\n",
    "\n",
    "llm = get_llm_client()\n",
    " \n",
    "system_prompt = system_prompt.format(user_enquiry=user_enquiry)\n",
    " \n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "                                            [\n",
    "                                                (\"system\", system_prompt),\n",
    "                                                (\"human\", \"{user_enquiry}\")\n",
    "                                            ]\n",
    "                                        )\n",
    " \n",
    " \n",
    "output = llm.invoke(prompt.format(user_enquiry = user_enquiry)).content\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IAC2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
